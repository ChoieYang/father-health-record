# library(bayesopt)
library(pROC) #計算AUC分數
library(doParallel)
#>> 1.讀取信用卡詐欺資料集 ====
data("GermanCredit")  # 這是一個標準的信用卡詐欺檢測數據，目標是預測借款人的信用狀態
mydata <- data.table(GermanCredit)
str(mydata) #1000x62 (Good佔7成；Bad佔3成)
## 這是二元分類問題， Class(信用評級)標記為"Good"(良好信用)或"Bad"(不良信用)
## 共有20個變數：
# Duration 貸款期限(月)
# Amount	貸款金額
# Age	年齡(歲)
# Housing	住房狀況(Own / Rent / For Free)
# Job	工作類別
# ForeignWorker	是否為外籍勞工(Yes / No)
# CheckingAccountStatus	活期帳戶狀態
# CreditHistory	信用歷史
# Purpose	貸款用途(如購車、教育等)
summary(mydata)
X <- as.matrix(subset(GermanCredit, select = -Class))  #特徵變數
dim(X) #1000 x 61
# 確保因子類別順序
levels(GermanCredit$Class)
# 正確轉換為 0 和 1
y <- ifelse(GermanCredit$Class == "Bad", 1, 0)  # Bad為詐欺(1),其他為非詐欺(0)
#>> 3.切割訓練集與測試集 ====
set.seed(42)
train_index <- createDataPartition(y, p = 0.8, list = FALSE) #分層抽樣(不要使用sample)，以確保training和test的0/1比例相同
X_train <- X[train_index,]
y_train <- y[train_index]
X_test <- X[-train_index,]
y_test <- y[-train_index]
#>> 4-1.模型預訓練 ====
#... 將資料轉換為 XGBoost 格式 ====
dtrain <- xgb.DMatrix(data = X_train, label = y_train)
dtest <- xgb.DMatrix(data = X_test, label = y_test)
params <- list(
objective = "binary:logistic", # 二元分類
eval_metric = "auc", # 以 AUC 作為評估指標
max_depth = 4,  # 樹的最大深度
eta = 0.1,         # 學習率(控制每棵樹對最終預測的影響，較小值（如 0.1）可提高泛化能力，但需較多迭代)
nrounds = 100,      # boosting 數量(XGBoost的提升樹(Boosting)訓練次數，較大值可能提升準確度，但也可能過擬合)
early_stopping_rounds = 10 # 提前停止條件(若驗證集 AUC 連續 10 輪未改善，則提前停止訓練，防止過擬合)
)
#... 訓練 ====
model <- xgb.train( #此函式來自xgboost，需使用xgb.DMatrix將資料做轉換
params = params,
data = dtrain,
nrounds = params$nrounds,
watchlist = list(train = dtrain, test = dtest),
early_stopping_rounds = params$early_stopping_rounds
)
#... 使用 ParBayesianOptimization 進行貝氏調參 ====
registerDoParallel(cores = 4) #設定 4 核心運算
## 定義目標函數（最大化 AUC）
optimize_xgb <- function(max_depth, eta, colsample_bytree, subsample, gamma, nrounds) {
# 設定 XGBoost 參數
params <- list(
objective = "binary:logistic",
eval_metric = "auc",
max_depth = as.integer(max_depth),
colsample_bytree = colsample_bytree,
subsample = subsample,
gamma = gamma,
eta = eta,
nrounds = as.integer(nrounds)
)
# 訓練 XGBoost
xgb_model <- xgb.cv(
params = params,
data = dtrain,
nrounds = as.integer(nrounds),
nfold = 5,
early_stopping_rounds = 10, #當驗證集的效能沒有提升 10 個回合時，就會提前停止。
verbose = 0 #預設為0，不會顯示詳細的訓練過程
)
# 取得最佳 AUC 分數
best_auc <- max(xgb_model$evaluation_log$test_auc_mean)
return(list(Score = best_auc))
}
## 執行
opt_results <- bayesOpt(
FUN = optimize_xgb,
bounds = list( #(注意)bounds參數格式應該定義為連續範圍，而不是離散數值
max_depth = c(3L, 9L), #<-表示範圍在3~9，不是指3和9兩個數字
eta = c(0.01, 0.1),
colsample_bytree = c(0.7, 0.8),
subsample = c(0.7, 0.8),
gamma = c(0, 0.2),
nrounds = c(50L, 200L) #訓練回合數
),
initPoints = 10,  # 表示貝氏優化開始前，先隨機抽 10 組參數 來建立代理模型。之後貝氏優化才會進行更精細的參數搜尋（iters.n次）。
iters.n = 20,  # iters.n 代表要執行多少次優化
parallel = TRUE  # 啟用平行運算
)
exists("xgb.cv", where = asNamespace("xgboost"))
xgboost::xgb.cv
.rs.restartR()
library(xgboost)
xgboost::xgb.cv
R.version.string
rm(list = ls())
gc()
library(xgboost)
library(bayesOpt)
library(remotes)
remotes::install_github("anotherSamWilson/bayesOpt")
install.packages("rBayesianOptimization")
library(rBayesianOptimization)
## 定義目標函數（最大化 AUC）
optimize_xgb <- function(max_depth, eta, colsample_bytree, subsample, gamma, nrounds) {
# 設定 XGBoost 參數
params <- list(
objective = "binary:logistic",
eval_metric = "auc",
max_depth = as.integer(max_depth),
colsample_bytree = colsample_bytree,
subsample = subsample,
gamma = gamma,
eta = eta,
nrounds = as.integer(nrounds)
)
# 訓練 XGBoost
xgb_model <- xgb.cv(
params = params,
data = dtrain,
nrounds = as.integer(nrounds),
nfold = 5,
early_stopping_rounds = 10, #當驗證集的效能沒有提升 10 個回合時，就會提前停止。
verbose = 0 #預設為0，不會顯示詳細的訓練過程
)
# 取得最佳 AUC 分數
best_auc <- max(xgb_model$evaluation_log$test_auc_mean)
return(list(Score = best_auc))
}
## 執行
opt_results <- bayesOpt(
FUN = optimize_xgb,
bounds = list( #(注意)bounds參數格式應該定義為連續範圍，而不是離散數值
max_depth = c(3L, 9L), #<-表示範圍在3~9，不是指3和9兩個數字
eta = c(0.01, 0.1),
colsample_bytree = c(0.7, 0.8),
subsample = c(0.7, 0.8),
gamma = c(0, 0.2),
nrounds = c(50L, 200L) #訓練回合數
),
initPoints = 10,  # 表示貝氏優化開始前，先隨機抽 10 組參數 來建立代理模型。之後貝氏優化才會進行更精細的參數搜尋（iters.n次）。
iters.n = 20,  # iters.n 代表要執行多少次優化
parallel = TRUE  # 啟用平行運算
)
library(bayesOpt)
## 執行
opt_results <- BayesianOptimization(
FUN = optimize_xgb,
bounds = list( #(注意)bounds參數格式應該定義為連續範圍，而不是離散數值
max_depth = c(3L, 9L), #<-表示範圍在3~9，不是指3和9兩個數字
eta = c(0.01, 0.1),
colsample_bytree = c(0.7, 0.8),
subsample = c(0.7, 0.8),
gamma = c(0, 0.2),
nrounds = c(50L, 200L) #訓練回合數
),
initPoints = 10,  # 表示貝氏優化開始前，先隨機抽 10 組參數 來建立代理模型。之後貝氏優化才會進行更精細的參數搜尋（iters.n次）。
iters.n = 20,  # iters.n 代表要執行多少次優化
parallel = TRUE  # 啟用平行運算
)
opt_result <- (
FUN = optimize_xgb,
opt_result <- BayesianOptimization(
FUN = optimize_xgb,
bounds = list(
max_depth = c(3L, 9L),
eta = c(0.01, 0.1),
colsample_bytree = c(0.7, 0.8),
subsample = c(0.7, 0.8),
gamma = c(0, 0.2),
nrounds = c(50L, 200L)
),
init_points = 10,
n_iter = 20,
parallel = TRUE
)
rm(list=ls())
gc()
setwd("D:/Joy-desktop/Documents/R_scripts")
#>> 載入所需套件 ====
library(xgboost) #資料需使用xgb.DMatrixc做轉換後才能執行
library(caret) #引用其資料集German Credit Data，也可以使用library(mlbench)
library(data.table)
library(ParBayesianOptimization) #貝氏調參法
library(rBayesianOptimization)
library(pROC) #計算AUC分數
library(doParallel)
#>> 1.讀取信用卡詐欺資料集 ====
data("GermanCredit")  # 這是一個標準的信用卡詐欺檢測數據，目標是預測借款人的信用狀態
mydata <- data.table(GermanCredit)
str(mydata) #1000x62 (Good佔7成；Bad佔3成)
## 這是二元分類問題， Class(信用評級)標記為"Good"(良好信用)或"Bad"(不良信用)
## 共有20個變數：
# Duration 貸款期限(月)
# Amount	貸款金額
# Age	年齡(歲)
# Housing	住房狀況(Own / Rent / For Free)
# Job	工作類別
# ForeignWorker	是否為外籍勞工(Yes / No)
# CheckingAccountStatus	活期帳戶狀態
# CreditHistory	信用歷史
# Purpose	貸款用途(如購車、教育等)
summary(mydata)
#>> 2.轉換特徵變數與標籤 ====
X <- as.matrix(subset(GermanCredit, select = -Class))  #特徵變數
dim(X) #1000 x 61
# 確保因子類別順序
levels(GermanCredit$Class)
# 正確轉換為 0 和 1
y <- ifelse(GermanCredit$Class == "Bad", 1, 0)  # Bad為詐欺(1),其他為非詐欺(0)
#>> 3.切割訓練集與測試集 ====
set.seed(42)
train_index <- createDataPartition(y, p = 0.8, list = FALSE) #分層抽樣(不要使用sample)，以確保training和test的0/1比例相同
X_train <- X[train_index,]
y_train <- y[train_index]
X_test <- X[-train_index,]
y_test <- y[-train_index]
#>> 4-1.模型預訓練 ====
#... 將資料轉換為 XGBoost 格式 ====
dtrain <- xgb.DMatrix(data = X_train, label = y_train)
dtest <- xgb.DMatrix(data = X_test, label = y_test)
#... 使用 ParBayesianOptimization 進行貝氏調參 ====
registerDoParallel(cores = 4) #設定 4 核心運算
## 定義目標函數（最大化 AUC）
optimize_xgb <- function(max_depth, eta, colsample_bytree, subsample, gamma, nrounds) {
# 設定 XGBoost 參數
params <- list(
objective = "binary:logistic",
eval_metric = "auc",
max_depth = as.integer(max_depth),
colsample_bytree = colsample_bytree,
subsample = subsample,
gamma = gamma,
eta = eta,
nrounds = as.integer(nrounds)
)
# 訓練 XGBoost
xgb_model <- xgb.cv(
params = params,
data = dtrain,
nrounds = as.integer(nrounds),
nfold = 5,
early_stopping_rounds = 10, #當驗證集的效能沒有提升 10 個回合時，就會提前停止。
verbose = 0 #預設為0，不會顯示詳細的訓練過程
)
# 取得最佳 AUC 分數
best_auc <- max(xgb_model$evaluation_log$test_auc_mean)
return(list(Score = best_auc))
}
opt_result <- BayesianOptimization(
FUN = optimize_xgb,
bounds = list(
max_depth = c(3L, 9L),
eta = c(0.01, 0.1),
colsample_bytree = c(0.7, 0.8),
subsample = c(0.7, 0.8),
gamma = c(0, 0.2),
nrounds = c(50L, 200L)
),
init_points = 10,
n_iter = 20,
parallel = TRUE
)
GPfit::GP_fit(X = Par_Mat[Rounds_Unique, ], Y = Value_Vec[Rounds_Unique])
#... 使用 ParBayesianOptimization 進行貝氏調參 ====
registerDoParallel(cores = 4) #設定 4 核心運算
## 定義目標函數（最大化 AUC）
optimize_xgb <- function(max_depth, eta, colsample_bytree, subsample, gamma, nrounds) {
# 設定 XGBoost 參數
params <- list(
objective = "binary:logistic",
eval_metric = "auc",
max_depth = as.integer(max_depth),
colsample_bytree = colsample_bytree,
subsample = subsample,
gamma = gamma,
eta = eta,
nrounds = as.integer(nrounds)
)
# 訓練 XGBoost
xgb_model <- xgb.cv(
params = params,
data = dtrain,
nrounds = as.integer(nrounds),
nfold = 5,
early_stopping_rounds = 10, #當驗證集的效能沒有提升 10 個回合時，就會提前停止。
verbose = 0 #預設為0，不會顯示詳細的訓練過程
)
# 取得最佳 AUC 分數
best_auc <- max(xgb_model$evaluation_log$test_auc_mean)
return(list(Score = best_auc))
}
## 執行
opt_results <- BayesianOptimization(
FUN = optimize_xgb,
bounds = list( #(注意)bounds參數格式應該定義為連續範圍，而不是離散數值
max_depth = c(3L, 9L), #<-表示範圍在3~9，不是指3和9兩個數字
eta = c(0.01, 0.1),
colsample_bytree = c(0.7, 0.8),
subsample = c(0.7, 0.8),
gamma = c(0, 0.2),
nrounds = c(50L, 200L) #訓練回合數
),
init_points = 10,  # 表示貝氏優化開始前，先隨機抽 10 組參數 來建立代理模型。之後貝氏優化才會進行更精細的參數搜尋（init_points次）。
n_iter = 20,  # n_iter 代表要執行多少次優化
)
#... 使用最佳參數來訓練最終的 XGBoost 模型，並對測試集進行評估 ====
(best_params <- getBestPars(opt_results))
print(opt_results)
#... 使用最佳參數來訓練最終的 XGBoost 模型，並對測試集進行評估 ====
(best_params <- opt_results$Best_Par)
print(best_params)
final_model <- xgboost(
data = dtrain,
objective = "binary:logistic",
eval_metric = "auc",
max_depth = as.integer(best_params$max_depth),
eta = best_params$eta,
colsample_bytree = best_params$colsample_bytree,
subsample = best_params$subsample,
gamma = best_params$gamma,
nrounds = as.integer(best_params$nrounds),
verbose = 1  # 訓練時顯示進度
)
best_params["max_depth"]
best_params[max_depth]
str(best)
str(best_params)
best_params["max_depth"]
best_params["max_depth",]
best_params[max_depth,]
str(opt_results$Best_Par)
data.table(opt_results$Best_Par)
save.image('stop.Rdata')
setwd("D:/Joy/爸爸的健康資訊/健康追蹤數據分析")
library(readr)
library(data.table)
library(ggplot2)
library(dplyr)
health_data <- read_csv("health_data.csv", show_col_types=F)
View(health_data)
health_data <- data.table(health_data)
str(health_data)
health_data[,c('date','status','label'):=list(
as.Date(date),
factor(status, levels=c('breakfast','lunch','dinner','sleep')),
factor(label, levels=c('before', 'after'))
)]
long_data <- melt(health_data, id.vars=1:4, variable.name='item', value.name='obs_value') %>% setDT
setorder(long_data, date, time)
label_setting <- c("SBP"="收縮壓", "DBP"="舒張壓", "pulse"="心跳", "Glucose"="血糖")
date1_breaks <- long_data %>%
filter(time == 1) %>%
pull(date) %>%
unique() %>%
sort() %>%
paste0("：1")
## (排除飯後)
ggplot() +
geom_point(data=long_data[label=='before'], aes(
x = paste0(date,'：',time), y = obs_value,
color=as.character(status)), size=3) +
geom_line(data = long_data[label=='before'], aes(
x = paste0(date,'：',time), y = obs_value), color='gray', size=1, group=1) +
geom_vline(xintercept = "2025-04-29：1", size=1, lty='dashed') +
# 新增趨勢線
# 趨勢線（所有 item 都畫一條線，每個 facet 內一條）
geom_smooth(
data   = long_data,
aes(
x     = paste0(date, "：", time),
y     = obs_value,
group = 1
),
method = "loess",    # 線性趨勢；改成 "loess" 即可畫局部平滑曲線
se     = FALSE,    # 不畫信賴區間
color = 'black',
size = 0.8
) +
facet_wrap(.~item, scales = 'free_y',
labeller = as_labeller(label_setting),
nrow=4) +
theme_bw() +
scale_x_discrete(
breaks = date1_breaks,
labels = gsub("2025-","",gsub("：1", "", date1_breaks))
) +
theme(strip.text = element_text(size = 12, face = 'bold'),
legend.title = element_text(size=12, face='bold'),
legend.text = element_text(size=12, face='bold'),
axis.title = element_text(size=12, face='bold'),
plot.subtitle = element_text(size=12, face='bold')) +
labs(x='觀察日與時間', y='測量值', color='時間點',
subtitle='4/29開始服用調整過的用藥') +
scale_color_manual(values = c(
"breakfast" = "red",
"lunch" = "orange",
"dinner" = "royalblue",
"sleep" = "green"
),
breaks = c('breakfast','lunch','dinner','sleep'),
labels = c('早餐前','午餐前','晚餐前','睡前'))  # 控制圖例順序
long_data[,type:=fifelse(date<as.Date('2025-04-29'),'用藥調整前','用藥調整後')]
ggplot(long_data[label=='before'], aes(x=obs_value, color=type)) +
geom_density(size=1) +
facet_wrap(.~item, scales = 'free',
labeller = as_labeller(label_setting)) +
theme_bw() +
labs(x='測量值',  title='調整用藥前後，各種測量值的分布',
subtitle='黑色是調整前；紅色是調整後', y='') +
scale_color_manual(values=c('black','orangered')) +
theme(strip.text = element_text(size = 12, face = 'bold'),
axis.title = element_text(size=12, face='bold'),
plot.subtitle = element_text(size=12, face='bold'),
plot.title = element_text(size=13, face='bold'),
legend.position = 'none')
setwd("D:/Joy/爸爸的健康資訊/健康追蹤數據分析")
library(readr)
library(data.table)
library(ggplot2)
library(dplyr)
health_data <- read_csv("health_data.csv", show_col_types=F)
View(health_data)
health_data <- data.table(health_data)
str(health_data)
health_data[,c('date','status','label'):=list(
as.Date(date),
factor(status, levels=c('breakfast','lunch','dinner','sleep')),
factor(label, levels=c('before', 'after'))
)]
long_data <- melt(health_data, id.vars=1:4, variable.name='item', value.name='obs_value') %>% setDT
setorder(long_data, date, time)
label_setting <- c("SBP"="收縮壓", "DBP"="舒張壓", "pulse"="心跳", "Glucose"="血糖")
# summary_data <- long_data %>%
#   group_by(item) %>%
#   summarize(
#     q1 = quantile(obs_value, probs=0.25, na.rm = TRUE),
#     median_value = median(obs_value, na.rm = TRUE),
#     mean_value = mean(obs_value, na.rm=T),
#     q3 = quantile(obs_value, probs=0.75, na.rm = TRUE))
## 只顯示所有日期的「日期：1」
date1_breaks <- long_data %>%
filter(time == 1) %>%
pull(date) %>%
unique() %>%
sort() %>%
paste0("：1")
## (排除飯後)
ggplot() +
geom_point(data=long_data[label=='before'], aes(
x = paste0(date,'：',time), y = obs_value,
color=as.character(status)), size=3) +
geom_line(data = long_data[label=='before'], aes(
x = paste0(date,'：',time), y = obs_value), color='gray', size=1, group=1) +
geom_vline(xintercept = "2025-04-29：1", size=1, lty='dashed') +
# 新增趨勢線
# 趨勢線（所有 item 都畫一條線，每個 facet 內一條）
geom_smooth(
data   = long_data,
aes(
x     = paste0(date, "：", time),
y     = obs_value,
group = 1
),
method = "loess",    # 線性趨勢；改成 "loess" 即可畫局部平滑曲線
se     = FALSE,    # 不畫信賴區間
color = 'black',
size = 0.8
) +
facet_wrap(.~item, scales = 'free_y',
labeller = as_labeller(label_setting),
nrow=4) +
theme_bw() +
scale_x_discrete(
breaks = date1_breaks,
labels = gsub("2025-","",gsub("：1", "", date1_breaks))
) +
theme(strip.text = element_text(size = 12, face = 'bold'),
legend.title = element_text(size=12, face='bold'),
legend.text = element_text(size=12, face='bold'),
axis.title = element_text(size=12, face='bold'),
plot.subtitle = element_text(size=12, face='bold')) +
labs(x='觀察日與時間', y='測量值', color='時間點',
subtitle='4/29開始服用調整過的用藥') +
scale_color_manual(values = c(
"breakfast" = "red",
"lunch" = "orange",
"dinner" = "royalblue",
"sleep" = "green"
),
breaks = c('breakfast','lunch','dinner','sleep'),
labels = c('早餐前','午餐前','晚餐前','睡前'))  # 控制圖例順序
long_data[,type:=fifelse(date<as.Date('2025-04-29'),'用藥調整前','用藥調整後')]
ggplot(long_data[label=='before'], aes(x=obs_value, color=type)) +
geom_density(size=1) +
facet_wrap(.~item, scales = 'free',
labeller = as_labeller(label_setting)) +
theme_bw() +
labs(x='測量值',  title='調整用藥前後，各種測量值的分布',
subtitle='黑色是調整前；紅色是調整後', y='') +
scale_color_manual(values=c('black','orangered')) +
theme(strip.text = element_text(size = 12, face = 'bold'),
axis.title = element_text(size=12, face='bold'),
plot.subtitle = element_text(size=12, face='bold'),
plot.title = element_text(size=13, face='bold'),
legend.position = 'none')
